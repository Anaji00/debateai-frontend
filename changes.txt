====================================================================
File: src/lib/stream.ts
====================================================================

// src/lib/stream.ts
// Strict NDJSON streaming decoder for fetch() Response bodies.
// - Uses TextDecoder(stream: true) so multibyte UTF-8 codepoints don't split
// - Carries partial lines between chunks
// - Emits ONLY complete, parsed JSON lines (no raw fragments)
// - Preserves spaces INSIDE JSON fields
// - Safely discards a single trailing '\r' (CR) added by \r\n line endings

export type StreamOptions = { signal?: AbortSignal; debug?: boolean };

/**
 * Processes a fetch() Response body as a stream of NDJSON (Newline Delimited JSON).
 * Each valid JSON line is parsed and passed to the `onEvent` callback.
 *
 * @param res The `Response` object from a `fetch` call.
 * @param onEvent The callback function to execute for each parsed JSON object.
 * @param opts Optional parameters, including an AbortSignal and a debug flag.
 */
export async function streamJSON<T = unknown>(
  res: Response,
  onEvent: (ev: T) => void,
  opts: StreamOptions = {}
) {
  // 1. Basic setup and error handling
  // Ensure the response is successful (status 200-299).
  if (!res.ok) {
    throw new Error(`HTTP ${res.status}: ${await safeText(res)}`);
  }
  // Get a reader to process the response body stream.
  const reader = res.body?.getReader();
  if (!reader) return;

  // Use TextDecoder to convert Uint8Array chunks into UTF-8 strings.
  // `fatal: false` prevents it from throwing on invalid UTF-8 sequences.
  const decoder = new TextDecoder("utf-8", { fatal: false });
  
  // `carry` stores any partial line from the end of a chunk to be prepended to the next.
  let carry = ""; 

  const { signal, debug = false } = opts;

  // 2. Helper function for parsing and event emission
  // This DRYs up the parsing logic used in the loop and at the end.
  function parseAndEmit(line: string) {
    // `trim()` handles trailing `\r` from `\r\n` and skips empty lines.
    const trimmed = line.trim();
    if (!trimmed) return;
    try {
      // Parse the line as JSON and call the user-provided callback.
      onEvent(JSON.parse(trimmed) as T);
    } catch (e) {
      // Log any JSON parsing errors if in debug mode.
      if (debug) console.warn("[streamJSON] drop bad JSON:", JSON.stringify(trimmed), e);
    }
  }

  // 3. AbortSignal handling
  // Allows the stream processing to be cancelled.
  let onAbort: (() => void) | undefined;
  if (signal) {
    if (signal.aborted) {
      try { await reader.cancel(); } catch {}
      return;
    }
    onAbort = () => { try { reader.cancel(); } catch {} };
    signal.addEventListener("abort", onAbort, { once: true });
  }

  try {
    // 4. Main processing loop
    while (true) {
      // Read the next chunk from the stream. `done` will be true when the stream ends.
      const { value, done } = await reader.read();
      if (done) break;

      // Decode the Uint8Array chunk to a string. `stream: true` is crucial
      // as it allows the decoder to handle multi-byte characters split across chunks.
      const chunk = decoder.decode(value, { stream: true });
      
      // Prepend any partial line from the previous chunk.
      const data = carry + chunk;
      
      // Split the data into lines based on the newline character.
      const lines = data.split('\n');

      // The last element of the split might be an incomplete line. We "carry" it
      // to the next iteration. If the chunk ends with a newline, `pop` returns
      // an empty string, which is correct.
      carry = lines.pop() || '';

      // Process each complete line.
      for (const line of lines) {
        parseAndEmit(line);
      }
    }

    // 5. Final flush
    // When the stream is done, flush the decoder for any remaining bytes.
    const tail = decoder.decode();
    if (tail) carry += tail;
    
    // If there's any remaining data in `carry`, it represents the final line. Parse it.
    if (carry) {
      parseAndEmit(carry);
    }

  } finally {
    // 6. Cleanup
    // Ensure the AbortSignal listener is removed and the reader is cancelled.
    if (signal && onAbort) signal.removeEventListener("abort", onAbort);
    try { await reader.cancel(); } catch {}
  }
}

/**
 * Safely reads the response body as text, returning a fallback on error.
 */
async function safeText(res: Response) {
  try { return await res.text(); } catch { return "<unreadable body>"; }
}

====================================================================
File: src/hooks/useDebate.ts
====================================================================

import { useEffect, useRef, useState } from "react";
import {
  versusStartStream,
  versusInjectStream,
  soloStartStream,
  soloInjectStream,
  daStartStream,
  daInjectStream,
  fetchSummary,
  fetchGrade,
  postVoice,
  type JsonlEvent,
  type ChatHistory,
} from "../api/debateClient";

import { sanitizeUserSeed} from "../lib/text";
import { resolveAudioPath } from "../lib/audio";
import {
  appendDeltaByTurnId,
  setSourcesByTurnId,
  updateAudioByTurnId,
  updateAudioOnLatestBySpeaker,
} from "../lib/messageOps";
import { SPEAKER_USER, type DebateModeKey, type Message } from "../lib/chat";

// NOTE: if your local debounce takes (fn, waitMs), you can add the wait back.
import {
  debounce,
  saveSession,
  loadSession,
  newSessionFromSeed,
  type StoredSession,
} from "../lib/persist";

import { detectTargetSpeakers, normalizeAddressees } from "../lib/targets";

type UseDebateArgs = { mode: DebateModeKey; userId: string };

export default function useDebate({ mode, userId }: UseDebateArgs) {
  // ---------------- state ----------------
  const [sessionId, setSessionId] = useState<string | null>(null);
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState("");

  const [c1, setC1] = useState("");
  const [c2, setC2] = useState("");

  const [started, setStarted] = useState(false);
  const [loading, setLoading] = useState(false);

  // For UI: character-aware typing indicator and caret target
  const [typingSpeaker, setTypingSpeaker] = useState<string | null>(null);
  const [streamingTurnId, setStreamingTurnId] = useState<number | null>(null);

  // ---------------- refs ----------------
  const scrollRef = useRef<HTMLDivElement | null>(null);
  const streamAbortRef = useRef<AbortController | null>(null);
  const ttsAbortRef = useRef<AbortController | null>(null);
  const audioCacheRef = useRef<Map<number, string>>(new Map());

  // Track first visible paint per turn, to optionally hide the indicator
  const firstDeltaSeenRef = useRef<Set<number>>(new Set());

  // Pre-roll dwell timer per turn
  const turnStartAtRef = useRef<Map<number, number>>(new Map());

  // Avoid stale closures: mirror streamingTurnId into a ref
  const streamingTurnIdRef = useRef<number | null>(null);
  useEffect(() => {
    streamingTurnIdRef.current = streamingTurnId;
  }, [streamingTurnId]);

  // Tuning knobs
  const MIN_TYPING_DWELL_MS = 120; // 0 => no dwell; try 80â€“150ms
  const HIDE_INDICATOR_ON_FIRST_PAINT = true;

  // ---------------- persist session ----------------
  const debouncedSave = useRef(
    debounce((s: StoredSession) => {
      saveSession(userId, s);
    })
  ).current;

  useEffect(() => {
    if (!sessionId) return;
    const firstUser = messages.find((m) => m.role === "user")?.text || "New chat";
    const existing = loadSession(userId, sessionId);
    const createdAt = existing?.createdAt || Date.now();
    const s: StoredSession = {
      id: sessionId,
      userId,
      title: firstUser.slice(0, 60),
      createdAt,
      updatedAt: Date.now(),
      messages,
    };
    debouncedSave(s);
  }, [messages, sessionId, userId, debouncedSave]);

  // ---------------- helpers ----------------
  function pushUser(text: string) {
    setMessages((prev) => [...prev, { role: "user", speaker: SPEAKER_USER, text }]);
  }

  function maybeAttachAnnotatedAudio(rawSpeaker: string, filename?: string | null) {
    if (!filename) return;
    const url = resolveAudioPath(rawSpeaker, filename);
    setMessages((prev) => updateAudioOnLatestBySpeaker(prev, rawSpeaker, url));
  }

  function buildHistory(): { chatml: ChatHistory } {
    const chatml: ChatHistory = messages.map((m) => ({
      role: m.role === "user" ? "user" : "assistant",
      content: m.text,
    }));
    return { chatml };
  }

  function buildAddressees(text: string): string[] | undefined {
    const candidates = [c1, c2].filter(Boolean);

    let targets: string[];
    const fn: any = detectTargetSpeakers as any;
    if (typeof fn === "function" && fn.length >= 2) {
      targets = fn(text, candidates);
    } else {
      targets = fn(text);
      if (Array.isArray(targets) && candidates.length) {
        targets = targets.filter((t: string) =>
          candidates.some((c) => c.toLowerCase() === String(t).toLowerCase())
        );
      }
    }
    const addrs = normalizeAddressees(targets);
    return addrs && addrs.length ? addrs : undefined;
  }

  // ---------------- event handler ----------------
  function onEvent(evt: JsonlEvent) {
    switch (evt.type) {
      case "session": {
        const id = String(evt.session_id);
        setSessionId(id);
        const firstUser = messages.find((m) => m.role === "user")?.text || "New chat";
        const base = newSessionFromSeed(userId, id, firstUser);
        saveSession(userId, { ...base, messages });
        break;
      }

      case "turn": {
        setLoading(true);
        setTypingSpeaker(evt.speaker || "Assistant");
        setStreamingTurnId(evt.turn_id);
        streamingTurnIdRef.current = evt.turn_id;

        // Start dwell timer for this turn
        turnStartAtRef.current.set(evt.turn_id, performance.now());
        firstDeltaSeenRef.current.delete(evt.turn_id);

        // Add empty assistant bubble to the UI
        setMessages((prev) => [
          ...prev,
          {
            role: "assistant",
            speaker: evt.speaker,
            text: "",
            turnId: evt.turn_id,
            sources: [],
          },
        ]);
        break;
      }

      case "sources": {
        setMessages((prev) => setSourcesByTurnId(prev, evt.turn_id, evt.items as any));
        break;
      }

      case "delta": {
        const activeId = streamingTurnIdRef.current;
        const isFirstDelta = activeId === evt.turn_id && !firstDeltaSeenRef.current.has(activeId);

        const applyDelta = () => {
          setMessages((prev) => appendDeltaByTurnId(prev, evt.turn_id, evt.delta));
          // If this was the first delta, mark it as seen and hide the typing indicator.
          if (isFirstDelta) {
            firstDeltaSeenRef.current.add(evt.turn_id);
            if (HIDE_INDICATOR_ON_FIRST_PAINT) {
              setTypingSpeaker(null);
            }
          }
        };

        // If it's the first delta, respect the dwell time to show the "thinking" animation.
        if (isFirstDelta) {
          const startedAt = turnStartAtRef.current.get(evt.turn_id) || 0;
          const elapsed = performance.now() - startedAt;
          if (elapsed < MIN_TYPING_DWELL_MS) {
            const remain = MIN_TYPING_DWELL_MS - elapsed;
            setTimeout(applyDelta, remain);
          } else {
            applyDelta(); // Dwell time already passed
          }
        } else {
          applyDelta(); // Subsequent deltas are applied immediately
        }

        // Optional inline audio annotation: [AUDIO::\<speaker>::\<filename>::]
        const m = evt.delta.match(/\\[AUDIO::([^:]+)::([^:]+)::\]/i);
        if (m) {
          const [, rawSpeaker, filename] = m;
          maybeAttachAnnotatedAudio(rawSpeaker, filename);
        }
        break;
      }

      case "endturn": {
        setLoading(false);

        if (streamingTurnIdRef.current === evt.turn_id) {
          setTypingSpeaker(null);
          setStreamingTurnId(null);
          streamingTurnIdRef.current = null;
        }
        turnStartAtRef.current.delete(evt.turn_id);

        if (sessionId) {
          const existing = loadSession(userId, sessionId);
          if (existing) {
            saveSession(userId, { ...existing, updatedAt: Date.now() });
          }
        }
        break;
      }

      case "error": {
        console.error("[stream error]", evt.message);
        setLoading(false);
        setTypingSpeaker(null);
        if (streamingTurnIdRef.current != null) {
          turnStartAtRef.current.delete(streamingTurnIdRef.current);
        }
        setStreamingTurnId(null);
        streamingTurnIdRef.current = null;
        break;
      }
    }
  }

  // ---------------- session adoption ----------------
  async function adoptSession(existingId: string) {
    const stored = loadSession(userId, existingId);
    if (!stored) return;
    setSessionId(existingId);
    setMessages(stored.messages || []);
    setStarted(true);
    setLoading(false);
  }

  // ---------------- actions ----------------
  async function start() {
    if (started) return;
    const seed = sanitizeUserSeed(input.trim(), [c1, c2].filter(Boolean));
    if (mode !== "Versus" && !seed) return;
    if (mode === "Versus" && (!seed || !c1 || !c2)) return;

    setStarted(true);
    pushUser(seed);
    setLoading(true);

    streamAbortRef.current?.abort();
    streamAbortRef.current = new AbortController();
    const signal = streamAbortRef.current.signal;

    const { chatml } = buildHistory();

    try {
      if (mode === "Versus") {
        await versusStartStream({ topic: seed, c1, c2, history: chatml, onEvent, signal });
      } else if (mode === "Solo") {
        await soloStartStream({ character: c1, topic: seed, history: chatml, onEvent, signal });
      } else {
        await daStartStream({ character: c1, thesis: seed, history: chatml, onEvent, signal });
      }
    } catch (e) {
      console.error("[stream start] failed:", e);
      setLoading(false);
      setTypingSpeaker(null);
      setStreamingTurnId(null);
      streamingTurnIdRef.current = null;
    }
  }

  async function send() {
    const text = input.trim();
    if (!text || !started || !sessionId) return;
    pushUser(text);
    setLoading(true);

    streamAbortRef.current?.abort();
    streamAbortRef.current = new AbortController();
    const signal = streamAbortRef.current.signal;

    try {
      if (mode === "Versus") {
        await versusInjectStream({
          session_id: Number(sessionId),
          user_inject: text,
          addressed_to: buildAddressees(text),
          onEvent,
          signal,
        });
      } else if (mode === "Solo") {
        await soloInjectStream({ session_id: Number(sessionId), user_inject: text, onEvent, signal });
      } else {
        await daInjectStream({ session_id: Number(sessionId), user_inject: text, onEvent, signal });
      }
    } catch (e) {
      console.error("[stream inject] failed:", e);
      setLoading(false);
      setTypingSpeaker(null);
      setStreamingTurnId(null);
      streamingTurnIdRef.current = null;
    }
  }

  function handlePrimary() {
    (started ? send : start)();
    setInput("");
  }

  async function summarize() {
    if (!sessionId) return;
    try {
      const data = await fetchSummary(Number(sessionId), "concise");
      setMessages((m) => [
        ...m,
        { role: "system", speaker: "Judge", text: "Summary:\n" + JSON.stringify((data as any).summary, null, 2) },
      ]);
    } catch (e) {
      console.error("[summary] failed:", e);
    }
  }

  async function grade() {
    if (!sessionId) return;
    try {
      const data = await fetchGrade(Number(sessionId));
      setMessages((m) => [
        ...m,
        {
          role: "system",
          speaker: "Judge",
          text: "Grade:\n" + JSON.stringify((data as any).grade ?? (data as any).grading, null, 2),
        },
      ]);
    } catch (e) {
      console.error("[grade] failed:", e);
    }
  }

  async function speak(turnId?: number) {
    if (!turnId) return;
    const cached = audioCacheRef.current.get(turnId);
    if (cached) {
      setMessages((prev) => updateAudioByTurnId(prev, turnId, cached));
      return;
    }

    ttsAbortRef.current?.abort();
    ttsAbortRef.current = new AbortController();

    try {
      const ready = await postVoice(turnId, ttsAbortRef.current.signal);
      const url = ready.audio_url || resolveAudioPath("", ready.filename || "");
      if (!url) throw new Error("No audio URL returned");
      setMessages((prev) => updateAudioByTurnId(prev, turnId, url));
      audioCacheRef.current.set(turnId, url);
    } catch (e) {
      console.error("[tts] failed:", e);
    }
  }

  function reset() {
    streamAbortRef.current?.abort();
    ttsAbortRef.current?.abort();
    setSessionId(null);
    setMessages([]);
    setInput("");
    setStarted(false);
    setLoading(false);
    setTypingSpeaker(null);
    setStreamingTurnId(null);
    streamingTurnIdRef.current = null;
    audioCacheRef.current.clear();
    firstDeltaSeenRef.current.clear();
    turnStartAtRef.current.clear();
  }

  function hardReset() {
    reset();
    setC1("");
    setC2("");
  }


  function onKeyDown(e: React.KeyboardEvent<HTMLTextAreaElement>) {
    if (e.key === "Enter" && !e.shiftKey) {
      e.preventDefault();
      handlePrimary();
    }
  }

  return {
    // reactive state for UI
    messages,
    input,
    setInput,
    c1,
    setC1,
    c2,
    setC2,
    started,
    loading,
    sessionId,

    // NEW: for MessageList typing indicator + caret
    typingSpeaker,
    streamingTurnId,

    // actions
    handlePrimary,
    summarize,
    grade,
    speak,
    reset,
    hardReset,

    adoptSession,
    onKeyDown,

    // scroll anchor if you auto-scroll elsewhere
    scrollRef,
  };
}
